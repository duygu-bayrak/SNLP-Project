{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document embeddings\n",
    "* Computing the position vectors using word2vec\n",
    "* Computing the document embeddings using doc2Vec\n",
    "* Computing the document embeddings using a subset of the methods proposed in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-4.0.1-cp38-cp38-manylinux1_x86_64.whl (23.9 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.9 MB 7.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=0.18.1 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.5.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /opt/conda/lib/python3.8/site-packages (from gensim) (1.19.5)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-5.0.0-py3-none-any.whl (56 kB)\n",
      "\u001b[K     |████████████████████████████████| 56 kB 2.3 MB/s  eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: smart-open, gensim\n",
      "Successfully installed gensim-4.0.1 smart-open-5.0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "skipTraining = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skipTraining:\n",
    "    from preprocessing import *\n",
    "    from cosine_sim import *\n",
    "    from synonym_enrich import *\n",
    "    from baseline import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skipTraining:\n",
    "    docs = parseDocs(\"cran/cran.all.1400\")\n",
    "    docs = tokenize_and_clean(docs)\n",
    "    docs = lemmatize(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 50\n",
    "if not skipTraining:\n",
    "\n",
    "    # sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n",
    "    # min_count: The minimum count of words to consider when training the model;\n",
    "    w2v_model = Word2Vec(docs, min_count=2, vector_size= vec_size, sg = 1)\n",
    "    w2v_model.save(\"w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skipTraining:\n",
    "    w2v_model = Word2Vec.load(\"w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01503546, -0.19407776,  0.28160238, -0.17297941, -0.4450978 ,\n",
       "       -0.10153174,  0.49798927,  0.28953522, -0.26928496, -0.3553878 ,\n",
       "        0.15962848, -0.11752975,  0.01413823, -0.05062268, -0.11307315,\n",
       "        0.01968868,  0.10698026,  0.06178449, -0.2272061 ,  0.11730936,\n",
       "        0.0034541 , -0.24787344,  0.6591028 ,  0.07714592,  0.20062515,\n",
       "        0.17791298, -0.26672757, -0.12411867, -0.7317228 , -0.0645718 ,\n",
       "       -0.32005432,  0.15800847,  0.03260247,  0.14398654,  0.08191456,\n",
       "       -0.42531094,  0.03000608,  0.07745912, -0.44342226, -0.16953444,\n",
       "        0.3397785 , -0.03678326,  0.26648775, -0.21699211,  0.71049553,\n",
       "       -0.11359502,  0.39135873, -0.24128321, -0.2730923 ,  0.48383117],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = w2v_model.wv['computer']  # get numpy vector of a word\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('digital', 0.9919171929359436),\n",
       " ('automatic', 0.9600348472595215),\n",
       " ('evaluation', 0.953161358833313),\n",
       " ('computing', 0.9528289437294006),\n",
       " ('complex', 0.9485465884208679),\n",
       " ('aid', 0.9430157542228699),\n",
       " ('formulation', 0.9394195675849915),\n",
       " ('suitable', 0.9387032389640808),\n",
       " ('practical', 0.9384737014770508),\n",
       " ('applying', 0.9383900761604309)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_docs, sorted_vocab = create_term_doc_matrix(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vanilla_pos_vec(w2v_model, tf_matrix, sorted_vocab, docs):\n",
    "    pos_vecs = []\n",
    "    for i, doc in enumerate(docs):\n",
    "        n_words = len(doc)\n",
    "        weighted_sum = np.zeros((vec_size,))\n",
    "        for word in doc:\n",
    "            if word in w2v_model.wv:\n",
    "                word_idx = sorted_vocab.index(word)\n",
    "                w = tf_matrix[word_idx][i]\n",
    "                weighted_sum += w * w2v_model.wv[word]\n",
    "\n",
    "        avg = weighted_sum/n_words\n",
    "        pos_vecs.append(avg)\n",
    "        \n",
    "    return pos_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1398"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_vecs = vanilla_pos_vec(w2v_model, tf_docs, sorted_vocab, docs)\n",
    "len(pos_vecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 50\n",
    "if not skipTraining:\n",
    "    tagged_docs = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(docs)]\n",
    "    d2v_model = Doc2Vec(tagged_docs, min_count=2, vector_size= vec_size)\n",
    "    d2v_model.save(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skipTraining:\n",
    "    d2v_model = Word2Vec.load(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = d2v_model.infer_vector(docs[0])\n",
    "v1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
