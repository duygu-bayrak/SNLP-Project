{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document embeddings\n",
    "* Computing the position vectors using word2vec\n",
    "* Computing the document embeddings using doc2Vec\n",
    "* Computing the document embeddings using a subset of the methods proposed in the review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in /opt/conda/lib/python3.7/site-packages (4.0.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.17.3)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (5.0.0)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /opt/conda/lib/python3.7/site-packages (from gensim) (1.3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade gensim\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "skipTraining = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skipTraining:\n",
    "    from preprocessing import *\n",
    "    from cosine_sim import *\n",
    "    from synonym_enrich import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not skipTraining:\n",
    "    docs = parseDocs(\"cran/cran.all.1400\")\n",
    "    docs = tokenize_and_clean(docs)\n",
    "    docs = lemmatize(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Position vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 50\n",
    "if not skipTraining:\n",
    "\n",
    "    # sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\n",
    "    # min_count: The minimum count of words to consider when training the model;\n",
    "    w2v_model = Word2Vec(docs, min_count=2, vector_size= vec_size, sg = 1)\n",
    "    w2v_model.save(\"w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skipTraining:\n",
    "    w2v_model = Word2Vec.load(\"w2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.16547997, -0.23902543,  0.34613046, -0.15685175, -0.33932897,\n",
       "       -0.14475358,  0.35155395,  0.5407517 , -0.12688686, -0.30697745,\n",
       "        0.12583013, -0.28550646,  0.14946432, -0.07647571, -0.21525793,\n",
       "        0.01549983,  0.016679  , -0.12470072, -0.2797611 ,  0.0069176 ,\n",
       "        0.15511991, -0.3263179 ,  0.6232955 , -0.064994  , -0.01753527,\n",
       "        0.05150538, -0.35302025, -0.2049817 , -0.63992643,  0.21758004,\n",
       "       -0.33853486,  0.0651901 , -0.04681535, -0.05625008,  0.00447547,\n",
       "       -0.3965827 ,  0.02142886,  0.03432292, -0.4676598 , -0.13731252,\n",
       "        0.6782075 , -0.04655042,  0.03315324, -0.18289414,  0.72673374,\n",
       "       -0.12769197,  0.27449325, -0.35474014, -0.2160631 ,  0.23292093],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector = w2v_model.wv['computer']  # get numpy vector of a word\n",
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('digital', 0.979901909828186),\n",
       " ('automatic', 0.9666916131973267),\n",
       " ('evaluation', 0.9542814493179321),\n",
       " ('computing', 0.9386852383613586),\n",
       " ('complex', 0.9368526339530945),\n",
       " ('practical', 0.9331501722335815),\n",
       " ('table', 0.9330777525901794),\n",
       " ('applying', 0.9317920207977295),\n",
       " ('formulation', 0.931194007396698),\n",
       " ('aid', 0.9302501082420349)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-9864add5d418>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m \u001b[0;31m#set weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;31m#needed for weighted averaging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mpos_vecs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "raise NotImplementedError #set weights\n",
    "w = 1 #needed for weighted averaging\n",
    "\n",
    "pos_vecs = []\n",
    "for doc in docs:\n",
    "    n_words = len(doc)\n",
    "    weighted_sum = np.zeros((vec_size,))\n",
    "    for word in doc:\n",
    "        if word in w2v_model.wv:\n",
    "            weighted_sum += w * w2v_model.wv[word]\n",
    "        \n",
    "    avg = weighted_sum/n_words\n",
    "    pos_vecs.append(avg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_size = 50\n",
    "if not skipTraining:\n",
    "    tagged_docs = [TaggedDocument(words=doc, tags=[str(i)]) for i, doc in enumerate(docs)]\n",
    "    d2v_model = Doc2Vec(tagged_docs, min_count=2, vector_size= vec_size)\n",
    "    d2v_model.save(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skipTraining:\n",
    "    d2v_model = Word2Vec.load(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19874685, -0.12971978,  0.147887  ,  0.26760656,  0.09977112,\n",
       "       -0.18255714,  0.14067312,  0.1964975 , -0.23258264, -0.07927153,\n",
       "        0.35591024,  0.03969877,  0.05411862, -0.02184379, -0.32557228,\n",
       "        0.02802402,  0.214599  , -0.66473687, -0.21687463, -0.13439621,\n",
       "        0.10326765,  0.02690773,  0.03022747, -0.0941508 ,  0.15157737,\n",
       "       -0.1088073 ,  0.02512744, -0.36371958, -0.2775667 , -0.12016089,\n",
       "       -0.14123534,  0.11724566,  0.03826113,  0.13443409, -0.16424286,\n",
       "        0.425728  ,  0.26057693, -0.16866845,  0.04461066, -0.02106278,\n",
       "        0.31882682,  0.25649196, -0.0412026 ,  0.09873915, -0.04683453,\n",
       "        0.06451657, -0.21350868, -0.1803241 ,  0.00790678,  0.31618693],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = d2v_model.infer_vector(docs[0])\n",
    "v1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
