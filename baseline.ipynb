{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline, we will be implementing a simple document retrieval system which \n",
    "* processes the texts (cleaning + removal of stop words + lemmatizing) then\n",
    "* represent the documents using the TF-IDF statistic and \n",
    "* perform the comparison using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocessing import *\n",
    "from cosine_sim import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_docs = parseDocs(\"cran/cran.all.1400\")\n",
    "base_docs = tokenize_and_clean(base_docs)\n",
    "base_docs = lemmatize(base_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def create_term_doc_matrix(base_docs):\n",
    "    \"\"\" Constructs a frequency term-document matrix\n",
    "    \n",
    "    this function takes in a list of documents and returns a term-document matrix\n",
    "    the rows are lemma types, the columns are documents \n",
    "    the rows should be sorted alphabetically\n",
    "    the order of the columns should be preserved as it's given in base_docs\n",
    "    the cell values are a number of times a lemma was seen in a document\n",
    "    the value should be zero, if a lemma is absent from a document\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    base_docs : a list of lists of strings [['a','a','b'], ['a','b','c']]\n",
    "        a list of documents represented as a list of lemmas\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    matrix : numpy array\n",
    "        a matrix where columns are documents and rows are lemma types,\n",
    "        the cells of the matrix contain lemma counts in a document,\n",
    "        the lemmas for rows are sorted alphabetically\n",
    "        for the example above it will be:\n",
    "            np.array([[2,1],\n",
    "                      [1,1],\n",
    "                      [0,1]])\n",
    "        \n",
    "    sorted_vocab : list of strings\n",
    "        a list of all the lemma types used in all documents (the rows of our matrix)\n",
    "        the words should be strings sorted alphabetically\n",
    "        for the example above it should be ['a','b','c']\n",
    "    \"\"\"\n",
    "    lemma_types = set()\n",
    "    count_list = []\n",
    "    \n",
    "    for doc in base_docs:\n",
    "        counter = dict(Counter(doc))\n",
    "        count_list.append(counter)\n",
    "        lemma_types = lemma_types.union(set(counter.keys()))\n",
    "\n",
    "    sorted_vocab = sorted(list(lemma_types))\n",
    "    rows = len(sorted_vocab)\n",
    "    columns = len(count_list)\n",
    "    matrix = np.zeros((rows,columns))\n",
    "    \n",
    "    for i, doc in enumerate(count_list):\n",
    "        for j, lemma in enumerate(sorted_vocab):\n",
    "            if lemma in doc.keys():\n",
    "                matrix[j][i] = doc[lemma]\n",
    "            else:\n",
    "                matrix[j][i] = 0\n",
    "                \n",
    "    return matrix, sorted_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_matrix, sorted_vocab = create_term_doc_matrix(base_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tf_idf(td_matrix):\n",
    "    \"\"\" Weighs a term-document matrix of raw counts with tf-idf scheme\n",
    "    \n",
    "    this function takes in a term-document matrix as a numpy array, \n",
    "    and weights the scores with the tf-idf algorithm described above.\n",
    "    idf values are modified with log_10\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    td_matrix : numpy array \n",
    "        a matrix where columns are documents and \n",
    "        rows are word counts in a document\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tf_idf_matrix : numpy array \n",
    "        a matrix where columns are documents and \n",
    "        rows are word tf-idf values in a document\n",
    "        \n",
    "    idf_vector : numpy array of shape (vocabulary-size, 1)\n",
    "        a vector of idf values for words in the collection. the shape is (vocabulary-size, 1)\n",
    "        this vector will be used to weight new query documents\n",
    "    \"\"\"\n",
    "    shape = td_matrix.shape\n",
    "    tf_idf_matrix = np.zeros(shape)\n",
    "    idf_vector = np.zeros((shape[0],1))\n",
    "    N = shape[1]\n",
    "    \n",
    "    for t, term in enumerate(td_matrix):\n",
    "        df_t = sum(x > 0 for x in term)\n",
    "        idf_t = math.log10(N/df_t)\n",
    "        idf_vector[t] = idf_t\n",
    "        for d, w_td in enumerate(term):\n",
    "            tf_idf_matrix[t][d] = w_td*idf_t\n",
    "            \n",
    "    \n",
    "    return tf_idf_matrix, idf_vector      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix, idf_vector = tf_idf(td_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsi(matrix, d):\n",
    "    \"\"\" Returns truncted SVD components\n",
    "    \n",
    "    this function takes in a term-document matrix, where\n",
    "    values can be both raw frequencies and weighted values (tf_idf)\n",
    "    and returns their trunctaded SVD matrices.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : numpy array\n",
    "        a numpy array where columns are documents and \n",
    "        rows are lemmas\n",
    "    d : int\n",
    "        a number of features we will be reducing our matrix to\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DT_d : numpy array\n",
    "        a [d x m], where m is the number of word dimensions in the original matrix, \n",
    "        and d is the number of features we want to keep\n",
    "        this is a matrix that represents documents with values for d hidden topics\n",
    "    transformation_matrix : numpy array \n",
    "        a matrix to transform queries into the same vector space as DT_d\n",
    "        T_dS_d^-, where S_d^- is inverse of S_d\n",
    "    \"\"\"\n",
    "    # Singular-value decomposition is already done for you\n",
    "    T, s, DT = np.linalg.svd(matrix)\n",
    "    S = np.diag(s)\n",
    "\n",
    "    DT_d = DT[0:d]\n",
    "    T_2 = np.array([t[0:d] for t in T])\n",
    "    S_2 = [x[0:d] for x in S[0:d]]\n",
    "    S_2_inv = np.linalg.inv(S_2)\n",
    "    transformation_matrix = np.dot(T_2,S_2_inv)\n",
    "\n",
    "    \n",
    "    return DT_d, transformation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "MemoryError",
     "evalue": "Unable to allocate 557. MiB for an array with shape (8543, 8543) and data type float64",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-68396aef63d7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Run the cell below to get dense 20d vector models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtf_idf_matrix_dense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_idf_matrix_transform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlsi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf_idf_matrix\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-a170be6f48b2>\u001b[0m in \u001b[0;36mlsi\u001b[1;34m(matrix, d)\u001b[0m\n\u001b[0;32m     26\u001b[0m     \"\"\"\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m# Singular-value decomposition is already done for you\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msvd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatrix\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m     \u001b[0mS\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msvd\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39-32\\lib\\site-packages\\numpy\\linalg\\linalg.py\u001b[0m in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1659\u001b[0m         \u001b[0msignature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'D->DdD'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misComplexType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'd->ddd'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1660\u001b[1;33m         \u001b[0mu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1661\u001b[0m         \u001b[0mu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1662\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_realType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 557. MiB for an array with shape (8543, 8543) and data type float64"
     ]
    }
   ],
   "source": [
    "# Run the cell below to get dense 20d vector models\n",
    "tf_idf_matrix_dense, tf_idf_matrix_transform = lsi(tf_idf_matrix,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(20, 1398)"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "tf_idf_matrix_dense.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_doc_matrix_queries(normalized_queries, sorted_vocabulary):\n",
    "    \"\"\" Constructs a frequency term-document matrix for queries\n",
    "    \n",
    "    this function takes in a list of queries and a vocabulary list and returns a term-document matrix\n",
    "    the rows are lemma types as given in vocabulary, the columns are documents\n",
    "    the rows should be in the same order as in vocabulary given\n",
    "    the order of the columns should be preserved as it's given in normalized_queries\n",
    "    the cell values are a number of times a lemma was seen in a document\n",
    "    the value should be zero, if a lemma is absent from a document\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    normalized_queries : a list of lists of strings [['a','a','b','d'], ['a','b','c']]\n",
    "        a list of queries represented as a list of lemmas\n",
    "    sorted_vocabulary : list of strings\n",
    "        a list of all the lemma types used in all training documents (the rows of our matrix)\n",
    "        the words are strings sorted alphabetically\n",
    "        for our example it will be ['a','b','c']\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    query_matrix : numpy array\n",
    "        a matrix where columns are documents in normalized_queries \n",
    "        and rows are lemma types from sorted_vocabulary.\n",
    "        for the example above it will be:\n",
    "            np.array([[2,1],\n",
    "                      [1,1],\n",
    "                      [0,1]])\n",
    "        'd' is not included in the matrix, because it is absent from sorted_vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    rows = len(sorted_vocabulary)\n",
    "    columns = len(normalized_queries)\n",
    "    query_matrix = np.zeros((rows,columns))\n",
    "    counter = [dict(Counter(query)) for query in normalized_queries]\n",
    "    \n",
    "    for i, query in enumerate(counter):\n",
    "        for j, lemma in enumerate(sorted_vocabulary):\n",
    "            if lemma in query.keys():\n",
    "                query_matrix[j][i] = query[lemma]\n",
    "            else:\n",
    "                query_matrix[j][i] = 0\n",
    "            \n",
    "    \n",
    "    return query_matrix   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_queries = parseDocs(\"cran/cran.qry\")\n",
    "normalized_queries = tokenize_and_clean(normalized_queries)\n",
    "normalized_queries = lemmatize(normalized_queries)\n",
    "\n",
    "# collect term-document matrix\n",
    "td_queries = create_term_doc_matrix_queries(normalized_queries, sorted_vocab)\n",
    "# weigh term-document matrix with tf-idf\n",
    "tf_idf_queries = td_queries*idf_vector \n",
    "# transform matrices with LSI\n",
    "tf_idf_queries_dense = tf_idf_queries.T.dot(tf_idf_matrix_transform).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "tf_idf_queries_dense.shape\n",
    "np.count_nonzero(tf_idf_queries.T[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "get 10 top:  [(nan, 0), (nan, 1), (nan, 2), (nan, 3), (nan, 4), (nan, 5), (nan, 6), (nan, 7), (nan, 8), (nan, 9)]\n",
      "c:\\Users\\user\\Desktop\\snlp_project\\cosine_sim.py:20: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  cos_sim = np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n"
     ]
    }
   ],
   "source": [
    "query = tf_idf_queries_dense.T[0] \n",
    "D = tf_idf_matrix_dense.T\n",
    "\n",
    "print(\"get 10 top: \", get_k_relevant(10, query, D))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python390jvsc74a57bd04f90bca6d5532465107dddf48f58200c9df515003323938e85648d01de9b439e",
   "display_name": "Python 3.9.0 32-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "metadata": {
   "interpreter": {
    "hash": "4f90bca6d5532465107dddf48f58200c9df515003323938e85648d01de9b439e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}