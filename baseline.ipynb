{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a baseline, we will be implementing a simple document retrieval system which \n",
    "* processes the texts (cleaning + removal of stop words + lemmatizing) then\n",
    "* represent the documents using the TF-IDF statistic and \n",
    "* perform the comparison using cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install ipynb\n",
    "#from ipynb.fs.full.preprocessing import parseDocs, tokenize_and_clean, lemmatize\n",
    "#from preprocessing import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from cosine_sim\n",
    "import numpy as np\n",
    "def cosine_sim(x, y):\n",
    "    \"\"\"calculates cosine similarity between 2 vectors.\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        vector representation (of query)\n",
    "    y : numpy.ndarray\n",
    "        vector representation (of document)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    cosine_sim: numpy.float64\n",
    "        cosine similarity between vector x and y\n",
    "    \"\"\"\n",
    "    \n",
    "    cos_sim = np.dot(x, y)/(np.linalg.norm(x)*np.linalg.norm(y))\n",
    "    \n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from cosine_sim\n",
    "def get_k_relevant(k, query, D):\n",
    "    \"\"\"returns ranked list of top k documents in descending order of their\n",
    "    cosine similarity with the query\n",
    "        \n",
    "    Parameters\n",
    "    ----------\n",
    "    k : int\n",
    "        number of documents to retrieve (top k)\n",
    "    query : numpy.ndarray\n",
    "        vector representation of query whose cosine similarity is to be computed with the corpus\n",
    "    D: list of numpy.ndarray\n",
    "        vector representation of all documents in corpus\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    ranked_sims: list of tuples (cosine similarity, index of document)\n",
    "        list of top k cosine similarities and the corresponding documents (their index) in descending order\n",
    "    \"\"\"\n",
    "      \n",
    "    cosine_sims = []\n",
    "    \n",
    "    for i, d in enumerate(D):\n",
    "        cosine_sims.append((cosine_sim(query, d), i))\n",
    "        \n",
    "    ranked_sims = sorted(cosine_sims, key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    if k != 0:\n",
    "        # if k=0 retrieve all documents in descending order\n",
    "        ranked_sims = ranked_sims[:k]\n",
    "    \n",
    "    return ranked_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/bayrakd1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/bayrakd1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/bayrakd1/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#from preprocessing file\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup #to remove HTML tags\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from preprocessing file\n",
    "def parseDocs(filename): \n",
    "    with open(filename,\"r\") as f:\n",
    "        docs = []\n",
    "        doc = \"\"\n",
    "        cont = False\n",
    "    \n",
    "        for line in f: #skip text between .I and .W\n",
    "            if \".I\" in line:\n",
    "                cont = False\n",
    "                if len(doc)>0:\n",
    "                    docs.append(doc)\n",
    "                    doc = \"\"\n",
    "            elif \".W\" in line:\n",
    "                cont = True\n",
    "            elif cont == True:\n",
    "                doc = doc + line\n",
    "\n",
    "        if len(doc)>0: #needed for the last document\n",
    "            docs.append(doc)\n",
    "\n",
    "        f.close()\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from preprocessing file\n",
    "def tokenize_and_clean(docs):\n",
    "    \"\"\"This function tokenizes texts into lowercased tokens with TreebankWordTokenizer\n",
    "    \n",
    "    Preprocesses the list of strings given as input.\n",
    "    Tokenize each string into sentences using sent_tokenize(),\n",
    "    tokenize each sentence into tokens using TreebankWordTokenizer().tokenize(),\n",
    "    Lowercasing the characters, removing non-ASCII values, special characters, HTML tags and stopwords.\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs : list of strings\n",
    "        list of document contents\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tokens : list of list of strings\n",
    "        each text as a list of lowercased tokens\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        # converting to lower case\n",
    "        txt = doc.lower()\n",
    "        \n",
    "        # remove HTML tags\n",
    "        txt = BeautifulSoup(txt, 'html.parser').get_text()\n",
    "        \n",
    "        # tokenize\n",
    "        sentence = sent_tokenize(txt)\n",
    "        tok = [TreebankWordTokenizer().tokenize(sent) for sent in sentence]\n",
    "        tok = [item for sublist in tok for item in sublist] #convert to one list\n",
    "        \n",
    "        # removing stop words and special characters from the tokens\n",
    "        clean_tokens = [word for word in tok if (word not in stop_list and not re.match('[^A-Za-z0-9]', word))]\n",
    "        \n",
    "        tokens.append(clean_tokens)\n",
    "\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from preprocessing file\n",
    "def lemmatize(doc_tokens):\n",
    "    \"\"\"This function lemmatizes texts with NLTK WordNetLemmatizer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_tokens : list of list of tokens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    doc_lemmas : list of list of lemmatized tokens\n",
    "    \"\"\"\n",
    "    doc_lemmas = []\n",
    "    \n",
    "    for doc in doc_tokens:\n",
    "        lemmas = [lemmatizer.lemmatize(token) for token in doc]\n",
    "        doc_lemmas.append(lemmas)\n",
    "        \n",
    "    return doc_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_docs = parseDocs(\"cran/cran.all.1400\")\n",
    "base_docs = tokenize_and_clean(base_docs)\n",
    "base_docs = lemmatize(base_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def create_term_doc_matrix(base_docs):\n",
    "    \"\"\" Constructs a frequency term-document matrix\n",
    "    \n",
    "    this function takes in a list of documents and returns a term-document matrix\n",
    "    the rows are lemma types, the columns are documents \n",
    "    the rows should be sorted alphabetically\n",
    "    the order of the columns should be preserved as it's given in base_docs\n",
    "    the cell values are a number of times a lemma was seen in a document\n",
    "    the value should be zero, if a lemma is absent from a document\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    base_docs : a list of lists of strings [['a','a','b'], ['a','b','c']]\n",
    "        a list of documents represented as a list of lemmas\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    matrix : numpy array\n",
    "        a matrix where columns are documents and rows are lemma types,\n",
    "        the cells of the matrix contain lemma counts in a document,\n",
    "        the lemmas for rows are sorted alphabetically\n",
    "        for the example above it will be:\n",
    "            np.array([[2,1],\n",
    "                      [1,1],\n",
    "                      [0,1]])\n",
    "        \n",
    "    sorted_vocab : list of strings\n",
    "        a list of all the lemma types used in all documents (the rows of our matrix)\n",
    "        the words should be strings sorted alphabetically\n",
    "        for the example above it should be ['a','b','c']\n",
    "    \"\"\"\n",
    "    lemma_types = set()\n",
    "    count_list = []\n",
    "    \n",
    "    for doc in base_docs:\n",
    "        counter = dict(Counter(doc))\n",
    "        count_list.append(counter)\n",
    "        lemma_types = lemma_types.union(set(counter.keys()))\n",
    "\n",
    "    sorted_vocab = sorted(list(lemma_types))\n",
    "    rows = len(sorted_vocab)\n",
    "    columns = len(count_list)\n",
    "    matrix = np.zeros((rows,columns))\n",
    "    \n",
    "    for i, doc in enumerate(count_list):\n",
    "        for j, lemma in enumerate(sorted_vocab):\n",
    "            if lemma in doc.keys():\n",
    "                matrix[j][i] = doc[lemma]\n",
    "            else:\n",
    "                matrix[j][i] = 0\n",
    "                \n",
    "    return matrix, sorted_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "td_matrix, sorted_vocab = create_term_doc_matrix(base_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def tf_idf(td_matrix):\n",
    "    \"\"\" Weighs a term-document matrix of raw counts with tf-idf scheme\n",
    "    \n",
    "    this function takes in a term-document matrix as a numpy array, \n",
    "    and weights the scores with the tf-idf algorithm described above.\n",
    "    idf values are modified with log_10\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    td_matrix : numpy array \n",
    "        a matrix where columns are documents and \n",
    "        rows are word counts in a document\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tf_idf_matrix : numpy array \n",
    "        a matrix where columns are documents and \n",
    "        rows are word tf-idf values in a document\n",
    "        \n",
    "    idf_vector : numpy array of shape (vocabulary-size, 1)\n",
    "        a vector of idf values for words in the collection. the shape is (vocabulary-size, 1)\n",
    "        this vector will be used to weight new query documents\n",
    "    \"\"\"\n",
    "    shape = td_matrix.shape\n",
    "    tf_idf_matrix = np.zeros(shape)\n",
    "    idf_vector = np.zeros((shape[0],1))\n",
    "    N = shape[1]\n",
    "    \n",
    "    for t, term in enumerate(td_matrix):\n",
    "        df_t = sum(x > 0 for x in term)\n",
    "        idf_t = math.log10(N/df_t)\n",
    "        idf_vector[t] = idf_t\n",
    "        for d, w_td in enumerate(term):\n",
    "            tf_idf_matrix[t][d] = w_td*idf_t\n",
    "            \n",
    "    \n",
    "    return tf_idf_matrix, idf_vector      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf_matrix, idf_vector = tf_idf(td_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lsi(matrix, d):\n",
    "    \"\"\" Returns truncted SVD components\n",
    "    \n",
    "    this function takes in a term-document matrix, where\n",
    "    values can be both raw frequencies and weighted values (tf_idf)\n",
    "    and returns their trunctaded SVD matrices.\n",
    "\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix : numpy array\n",
    "        a numpy array where columns are documents and \n",
    "        rows are lemmas\n",
    "    d : int\n",
    "        a number of features we will be reducing our matrix to\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    DT_d : numpy array\n",
    "        a [d x m], where m is the number of word dimensions in the original matrix, \n",
    "        and d is the number of features we want to keep\n",
    "        this is a matrix that represents documents with values for d hidden topics\n",
    "    transformation_matrix : numpy array \n",
    "        a matrix to transform queries into the same vector space as DT_d\n",
    "        T_dS_d^-, where S_d^- is inverse of S_d\n",
    "    \"\"\"\n",
    "    # Singular-value decomposition is already done for you\n",
    "    T, s, DT = np.linalg.svd(matrix)\n",
    "    S = np.diag(s)\n",
    "\n",
    "    DT_d = DT[0:d]\n",
    "    T_2 = np.array([t[0:d] for t in T])\n",
    "    S_2 = [x[0:d] for x in S[0:d]]\n",
    "    S_2_inv = np.linalg.inv(S_2)\n",
    "    transformation_matrix = np.dot(T_2,S_2_inv)\n",
    "\n",
    "    \n",
    "    return DT_d, transformation_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the cell below to get dense 20d vector models\n",
    "tf_idf_matrix_dense, tf_idf_matrix_transform = lsi(tf_idf_matrix,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1398)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_matrix_dense.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_doc_matrix_queries(normalized_queries, sorted_vocabulary):\n",
    "    \"\"\" Constructs a frequency term-document matrix for queries\n",
    "    \n",
    "    this function takes in a list of queries and a vocabulary list and returns a term-document matrix\n",
    "    the rows are lemma types as given in vocabulary, the columns are documents\n",
    "    the rows should be in the same order as in vocabulary given\n",
    "    the order of the columns should be preserved as it's given in normalized_queries\n",
    "    the cell values are a number of times a lemma was seen in a document\n",
    "    the value should be zero, if a lemma is absent from a document\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    normalized_queries : a list of lists of strings [['a','a','b','d'], ['a','b','c']]\n",
    "        a list of queries represented as a list of lemmas\n",
    "    sorted_vocabulary : list of strings\n",
    "        a list of all the lemma types used in all training documents (the rows of our matrix)\n",
    "        the words are strings sorted alphabetically\n",
    "        for our example it will be ['a','b','c']\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    query_matrix : numpy array\n",
    "        a matrix where columns are documents in normalized_queries \n",
    "        and rows are lemma types from sorted_vocabulary.\n",
    "        for the example above it will be:\n",
    "            np.array([[2,1],\n",
    "                      [1,1],\n",
    "                      [0,1]])\n",
    "        'd' is not included in the matrix, because it is absent from sorted_vocabulary\n",
    "    \"\"\"\n",
    "\n",
    "    rows = len(sorted_vocabulary)\n",
    "    columns = len(normalized_queries)\n",
    "    query_matrix = np.zeros((rows,columns))\n",
    "    counter = [dict(Counter(query)) for query in normalized_queries]\n",
    "    \n",
    "    for i, query in enumerate(counter):\n",
    "        for j, lemma in enumerate(sorted_vocabulary):\n",
    "            if lemma in query.keys():\n",
    "                query_matrix[j][i] = query[lemma]\n",
    "            else:\n",
    "                query_matrix[j][i] = 0\n",
    "            \n",
    "    \n",
    "    return query_matrix   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_queries = parseDocs(\"cran/cran.qry\")\n",
    "normalized_queries = tokenize_and_clean(normalized_queries)\n",
    "normalized_queries = lemmatize(normalized_queries)\n",
    "\n",
    "# collect term-document matrix\n",
    "td_queries = create_term_doc_matrix_queries(normalized_queries, sorted_vocab)\n",
    "# weigh term-document matrix with tf-idf\n",
    "tf_idf_queries = td_queries*idf_vector \n",
    "# transform matrices with LSI\n",
    "tf_idf_queries_dense = tf_idf_queries.T.dot(tf_idf_matrix_transform).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 225)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_queries_dense.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get 10 top:  [(0.7809579117962733, 50), (0.7676650546661836, 11), (0.7643710468981361, 252), (0.7618345019991778, 1166), (0.7376461569764691, 882), (0.7293633705409656, 1165), (0.729238984321086, 808), (0.7182758380584934, 1325), (0.7170035653923107, 99), (0.7074827920113113, 1302)]\n"
     ]
    }
   ],
   "source": [
    "query = tf_idf_queries_dense.T[0] \n",
    "D = tf_idf_matrix_dense.T\n",
    "\n",
    "print(\"get 10 top: \", get_k_relevant(10, query, D))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
