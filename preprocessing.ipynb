{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-Processing\n",
    "We will be studying the effect of preprocessing on the relevancy of the results. In other terms, the system will be tested with three preprocessing models, mainly:\n",
    "* Preprocessing model 1: cleaning + tokenization + removal of stop words \n",
    "* Preprocessing model 2: Model 1 + lemmatization\n",
    "* Preprocessing model 3: Model 1 + lemmatization +synonym enrichment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to\n[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package wordnet to\n[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Importing libraries\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import sent_tokenize, TreebankWordTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup #to remove HTML tags\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_list = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Documents in \"cran.all.1400\" look like this: \n",
    "\n",
    "**.I 1** (index) \\\n",
    "**.T** (Title) \\\n",
    "experimental investigation of the aerodynamics of a\n",
    "wing in a slipstream .\\\n",
    "**.A** (Author) \\\n",
    "brenckman,m.\\\n",
    "**.B** (?)\\\n",
    "j. ae. scs. 25, 1958, 324. \\\n",
    "**.W** (content)\\\n",
    "experimental investigation of the aerodynamics of a\n",
    "wing in a slipstream .\n",
    "  an experimental study of a wing in a propeller slipstream was\n",
    "made in order to determine the spanwise distribution of the lift\n",
    "increase due to slipstream at different angles of attack of the wing\n",
    "and at different free stream to slipstream velocity ratios .  the\n",
    "results were intended in part as an evaluation basis for different\n",
    "theoretical treatments of this problem .\n",
    "  the comparative span loading curves, together with\n",
    "supporting evidence, showed that a substantial part of the lift increment\n",
    "produced by the slipstream was due to a /destalling/ or\n",
    "boundary-layer-control effect .  the integrated remaining lift\n",
    "increment, after subtracting this destalling lift, was found to agree\n",
    "well with a potential flow theory .\n",
    "  an empirical evaluation of the destalling effects was made for\n",
    "the specific configuration of the experiment .\\\n",
    "**.I 2**\\\n",
    "...\n",
    "\n",
    "#### Queries in \"car.qry\" look like this:\n",
    "\n",
    "**.I** 001 \\\n",
    "**.W**\\\n",
    "what similarity laws must be obeyed when constructing aeroelastic models\n",
    "of heated high speed aircraft .\\\n",
    "**.I** 002\\\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseDocs(filename): \n",
    "    with open(filename,\"r\") as f:\n",
    "        docs = []\n",
    "        doc = \"\"\n",
    "        cont = False\n",
    "    \n",
    "        for line in f: #skip text between .I and .W\n",
    "            if \".I\" in line:\n",
    "                cont = False\n",
    "                if len(doc)>0:\n",
    "                    docs.append(doc)\n",
    "                    doc = \"\"\n",
    "            elif \".W\" in line:\n",
    "                cont = True\n",
    "            elif cont == True:\n",
    "                doc = doc + line\n",
    "            \n",
    "        if len(doc)>0: #needed for the last document\n",
    "            docs.append(doc)\n",
    "\n",
    "        f.close()\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "one-dimensional transient heat flow in a multilayer\nslab .\n  in a recent contribution to the readers'\nforum wassermann gave analytic\nsolutions for the temperature in a double\nlayer slab, with a triangular heat\nrate input at one face, insulated at the other,\nand with no thermal resistance\nat the interface .  his solutions were for the\nthree particular cases..\ni propose here to give the general solution\nto this problem, to indicate\nbriefly how it is obtained using the method of\nreference 2, and to point out\nthat the solutions given by wassermann are\nincomplete for times longer\nthan the duration of the heat input .\n\nnumber of documents:  1398\nwhat theoretical and experimental guides do we have as to turbulent\ncouette flow behaviour .\n\nnumber of queries:  225\n"
     ]
    }
   ],
   "source": [
    "docs = parseDocs(\"cran/cran.all.1400\")\n",
    "print(docs[5])\n",
    "print(\"number of documents: \",len(docs))\n",
    "queries = parseDocs(\"cran/cran.qry\")\n",
    "print(queries[5])\n",
    "print(\"number of queries: \",len(queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_clean(docs):\n",
    "    \"\"\"This function tokenizes texts into lowercased tokens with TreebankWordTokenizer\n",
    "    \n",
    "    Preprocesses the list of strings given as input.\n",
    "    Tokenize each string into sentences using sent_tokenize(),\n",
    "    tokenize each sentence into tokens using TreebankWordTokenizer().tokenize(),\n",
    "    Lowercasing the characters, removing non-ASCII values, special characters, HTML tags and stopwords.\n",
    "    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docs : list of strings\n",
    "        list of document contents\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    tokens : list of list of strings\n",
    "        each text as a list of lowercased tokens\n",
    "    \"\"\"\n",
    "    tokens = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        # converting to lower case\n",
    "        txt = doc.lower()\n",
    "        \n",
    "        # remove HTML tags\n",
    "        txt = BeautifulSoup(txt, 'html.parser').get_text()\n",
    "        \n",
    "        # tokenize\n",
    "        sentence = sent_tokenize(txt)\n",
    "        tok = [TreebankWordTokenizer().tokenize(sent) for sent in sentence]\n",
    "        tok = [item for sublist in tok for item in sublist] #convert to one list\n",
    "        \n",
    "        # removing stop words and special characters from the tokens\n",
    "        clean_tokens = [word for word in tok if (word not in stop_list and not re.match('[^A-Za-z0-9]', word))]\n",
    "        \n",
    "        tokens.append(clean_tokens)\n",
    "\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['one-dimensional', 'transient', 'heat', 'flow', 'multilayer', 'slab', 'recent', 'contribution', \"readers'\", 'forum', 'wassermann', 'gave', 'analytic', 'solutions', 'temperature', 'double', 'layer', 'slab', 'triangular', 'heat', 'rate', 'input', 'one', 'face', 'insulated', 'thermal', 'resistance', 'interface', 'solutions', 'three', 'particular', 'cases..', 'propose', 'give', 'general', 'solution', 'problem', 'indicate', 'briefly', 'obtained', 'using', 'method', 'reference', '2', 'point', 'solutions', 'given', 'wassermann', 'incomplete', 'times', 'longer', 'duration', 'heat', 'input']\n"
     ]
    }
   ],
   "source": [
    "doc_tokens = tokenize_and_clean(docs)\n",
    "print(doc_tokens[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['theoretical', 'experimental', 'guides', 'turbulent', 'couette', 'flow', 'behaviour']\n"
     ]
    }
   ],
   "source": [
    "query_tokens = tokenize_and_clean(queries)\n",
    "print(query_tokens[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(doc_tokens):\n",
    "    \"\"\"This function lemmatizes texts with NLTK WordNetLemmatizer\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_tokens : list of list of tokens\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    doc_lemmas : list of list of lemmatized tokens\n",
    "    \"\"\"\n",
    "    doc_lemmas = []\n",
    "    \n",
    "    for doc in doc_tokens:\n",
    "        lemmas = [lemmatizer.lemmatize(token) for token in doc]\n",
    "        doc_lemmas.append(lemmas)\n",
    "        \n",
    "    return doc_lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['one-dimensional', 'transient', 'heat', 'flow', 'multilayer', 'slab', 'recent', 'contribution', \"readers'\", 'forum', 'wassermann', 'gave', 'analytic', 'solution', 'temperature', 'double', 'layer', 'slab', 'triangular', 'heat', 'rate', 'input', 'one', 'face', 'insulated', 'thermal', 'resistance', 'interface', 'solution', 'three', 'particular', 'cases..', 'propose', 'give', 'general', 'solution', 'problem', 'indicate', 'briefly', 'obtained', 'using', 'method', 'reference', '2', 'point', 'solution', 'given', 'wassermann', 'incomplete', 'time', 'longer', 'duration', 'heat', 'input']\n"
     ]
    }
   ],
   "source": [
    "doc_lemmas = lemmatize(doc_tokens)\n",
    "print(doc_lemmas[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['theoretical', 'experimental', 'guide', 'turbulent', 'couette', 'flow', 'behaviour']\n"
     ]
    }
   ],
   "source": [
    "query_lemmas = lemmatize(query_tokens)\n",
    "print(query_lemmas[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_enrichment(doc_lemmas):\n",
    "    \"\"\"This function enriches the documents using a semantic knowledge base such as wordNet\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_lemmas : list of list of strings (lemmas)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    doc_enrich : list of list of strings\n",
    "    \"\"\"\n",
    "    doc_enrich = []\n",
    "    \n",
    "    for doc in doc_lemmas:\n",
    "        for lemma in doc:\n",
    "            for syn in wordnet.synsets(lemma):\n",
    "                #????\n",
    "                raise NotImplementedError()\n",
    "                    \n",
    "    return doc_enrich"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[Synset('transient.n.01'),\n",
       " Synset('transient.n.02'),\n",
       " Synset('transeunt.a.01'),\n",
       " Synset('ephemeral.s.01')]"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "wordnet.synsets(doc_lemmas[5][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(t): \n",
    "    return list(set(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"readers'\", 'resistance', 'multilayer', 'general', 'layer', 'insulated', 'duration', 'forum', 'indicate', 'heat', 'point', 'slab', 'recent', 'solution', 'double', 'face', 'give', 'gave', 'obtained', 'analytic', '2', 'propose', 'wassermann', 'one', 'incomplete', 'triangular', 'transient', 'interface', 'time', 'flow', 'three', 'cases..', 'briefly', 'using', 'longer', 'particular', 'problem', 'thermal', 'given', 'rate', 'one-dimensional', 'contribution', 'input', 'method', 'reference', 'temperature']\n"
     ]
    }
   ],
   "source": [
    "print(remove_duplicates(doc_lemmas[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get 3 pre-processing models:\n",
    "* Preprocessing model 1: cleaning + tokenization + removal of stop words \n",
    "* Preprocessing model 2: Model 1 + lemmatization\n",
    "* Preprocessing model 3: Model 1 + lemmatization +synonym enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-cbacff141a88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_and_clean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msynonym_enrichment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-92-4b482ef62739>\u001b[0m in \u001b[0;36msynonym_enrichment\u001b[0;34m(doc_lemmas)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0msyn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0;31m#????\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdoc_enrich\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "docs = parseDocs(\"cran/cran.all.1400\")\n",
    "model1 = tokenize_and_clean(docs)\n",
    "model2 = lemmatize(model1)\n",
    "model3 = synonym_enrichment(model2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python390jvsc74a57bd04f90bca6d5532465107dddf48f58200c9df515003323938e85648d01de9b439e",
   "display_name": "Python 3.9.0 32-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "metadata": {
   "interpreter": {
    "hash": "4f90bca6d5532465107dddf48f58200c9df515003323938e85648d01de9b439e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}