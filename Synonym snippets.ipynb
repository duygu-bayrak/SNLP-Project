{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supported-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "def synonym_mapping(doc_lemmas):\n",
    "    \"\"\"This function computes the synonym mappings of the terms in the vocabulary.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_lemmas : list of list of strings (lemmas)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mapping : dictionary mapping a term to a list of synonyms in the vocabulary\n",
    "    \"\"\"\n",
    "    def getSynonyms(term): #uses wordNet to get the synonym of every term\n",
    "        syns =[]\n",
    "        for synset in wordnet.synsets(term):\n",
    "            for lemma in synset.lemmas():\n",
    "                syns.append(lemma.name())\n",
    "        return list(set(syns +[term]))\n",
    "\n",
    "    #get the vocabulary\n",
    "    vocabulary =  []\n",
    "    for doc in doc_lemmas:\n",
    "        vocabulary += doc\n",
    "    vocabulary = list(set(vocabulary))\n",
    "\n",
    "    # get the mapping between each term of the vocabulary and a list of synonyms from wordNet\n",
    "    mapping ={vocabulary[i]: getSynonyms(vocabulary[i]) for i in range(len(vocabulary)) }\n",
    "     \n",
    "    #filter synonyms to only keep the ones in the vocabulary \n",
    "    mapping ={k: [ term for term in mapping[k] if term in vocabulary] for k in mapping.keys() }\n",
    "    \n",
    "    return [vocabulary, mapping]\n",
    "\n",
    "def synonym_enrichment_v1(doc_lemmas,mapping):\n",
    "    \"\"\"This function enriches the documents using a semantic knowledge base such as wordNet\n",
    "    \n",
    "    It adds the synonyms of each term to the document to enrich it\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_lemmas : list of list of strings (lemmas)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    doc_enrich : list of list of strings\n",
    "    \"\"\"\n",
    "    doc_enrich = []\n",
    "    \n",
    "    for doc in doc_lemmas:\n",
    "        enriched_doc =copy.deepcopy(doc)\n",
    "        for lemma in doc:\n",
    "            enriched_doc += mapping[lemma]\n",
    "        doc_enrich.append(enriched_doc)\n",
    "            \n",
    "              \n",
    "    return doc_enrich\n",
    "\n",
    "def synonym_enrichment_v2(doc_lemmas,mapping):\n",
    "    \"\"\"This function enriches the documents using a semantic knowledge base such as wordNet\n",
    "    \n",
    "    It adds the synonyms of each term to the document to enrich it\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    doc_lemmas : list of list of strings (lemmas)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    doc_enrich : list of list of strings\n",
    "    \"\"\"\n",
    "    \n",
    "#     dog = wn.synset('dog.n.01')\n",
    "#     cat = wn.synset('cat.n.01')\n",
    "\n",
    "#     print(dog.path_similarity(cat))\n",
    "#     print(dog.lch_similarity(cat))\n",
    "#     print(dog.wup_similarity(cat))\n",
    "\n",
    "    doc_enrich = []\n",
    "    for doc in doc_lemmas:\n",
    "        enriched_doc = Counter(doc)\n",
    "        \n",
    "        for lemma in doc:\n",
    "            #lemma has a freq of enriched_doc[lemma]\n",
    "            \n",
    "            for v in mapping[lemma]:\n",
    "                if v not in enriched_doc.keys():\n",
    "                    enriched_doc[v] =0\n",
    "                enriched_doc[v] +=  enriched_doc[lemma]*1/len(mapping[lemma])  #similarities(lemma,v) is too expensive... => attenuate\n",
    "        doc_enrich.append(enriched_doc)\n",
    "            \n",
    "            \n",
    "    return doc_enrich\n",
    "\n",
    "\n",
    "def synonym_enrichment_v3(doc_lemmas,mapping):\n",
    "    NewTokens = {k: [] for k in mapping.keys()}\n",
    "    for k in mapping.keys():\n",
    "        NewTokens[k].append(\"_\".join(mapping[k]))\n",
    "\n",
    "        for token in mapping[k]:\n",
    "            NewTokens[token].append(\"_\".join(mapping[k]))\n",
    "\n",
    "    NewTokens= {k: list(set(NewTokens[k])) for k in mapping.keys()}\n",
    "    \n",
    "    doc_enrich=[]\n",
    "    for doc in doc_lemmas:\n",
    "        doc_en = []\n",
    "        for term in doc:\n",
    "            doc_en +=NewTokens[term] \n",
    "        doc_enrich.append(doc_en)\n",
    "    return doc_enrich\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggressive-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = parseDocs(\"cran/cran.all.1400\")\n",
    "model1 = tokenize_and_clean(docs)\n",
    "model2 = lemmatize(model1)\n",
    "\n",
    "[vocabulary, mapping] = synonym_mapping(model2)\n",
    "model3 = synonym_enrichment_v1(model2, mapping)\n",
    "model4 = synonym_enrichment_v2(model2,mapping)\n",
    "\n",
    "df = pd.DataFrame(model4).fillna(0)\n",
    "\n",
    "model5 = synonym_enrichment_v3(model2,mapping)\n",
    "model5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
